{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting csv data from data/sp10/\n",
      "['AAPL.csv', 'ADSK.csv', 'EBAY.csv', 'FB.csv', 'GOOGL.csv', 'INTC.csv', 'INTU.csv', 'NFLX.csv', 'ORCL.csv', 'SYMC.csv']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import dataUtil\n",
    "\n",
    "N = 10 # depend on the N previous periods\n",
    "B = 1 # batch size\n",
    "\n",
    "dateSelected, stockPrice  = dataUtil.getData()\n",
    "logreturn = dataUtil.logReturn(stockPrice)\n",
    "# return for N previous periods, input, shape: [-1,K,N]\n",
    "logReturn_x = dataUtil.logReturnMatrix(logreturn, N)\n",
    "# return for current period\n",
    "logReturn_x0 = logreturn[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = len(stockPrice[0]) # K stocks\n",
    "logReturn_x_data = tf.reshape(logReturn_x, [-1,K,N,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, K, N])\n",
    "x_data = tf.reshape(x, [-1,K,N,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, K])\n",
    "previousPortfolio = tf.placeholder(tf.float32, shape=[None, K]) # portfolio for last time step\n",
    "previousReturn = tf.placeholder(tf.float32, shape=[None]) # return for the last time step\n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolution and pooling\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_4x1(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first convolution layer\n",
    "W_conv1 = weight_variable([1, 1, 1, 10])\n",
    "b_conv1 = bias_variable([10])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_data, W_conv1) + b_conv1)\n",
    "h_pool1 = avg_pool_4x1(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 10, 5)\n"
     ]
    }
   ],
   "source": [
    "# second convolutional layer\n",
    "W_conv2 = weight_variable([1, 1, 10, 5])\n",
    "b_conv2 = bias_variable([5])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = avg_pool_4x1(h_conv2)\n",
    "print h_pool2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# densely connected layer 1\n",
    "# wsize1 = int(h_pool2.shape[2] * h_pool2.shape[3])\n",
    "wsize1 = int(x_data.shape[2] * x_data.shape[3])\n",
    "wsize2 = wsize1\n",
    "W_fc1 = weight_variable([wsize1, wsize2] )\n",
    "prePort = tf.reshape(previousPortfolio,[-1,1])\n",
    "W_fc12 = weight_variable([1, wsize2])\n",
    "\n",
    "b_fc1 = bias_variable([wsize2])\n",
    "# h_pool2_flat = tf.reshape(h_pool2, [-1, wsize1])\n",
    "x_data_flat = tf.reshape(x_data, [-1, wsize1])\n",
    "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + tf.matmul(prePort, W_fc12) + b_fc1)\n",
    "h_fc1 = tf.nn.relu(tf.matmul(x_data_flat, W_fc1) + tf.matmul(prePort, W_fc12) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# densely connected layer 2 \n",
    "# readout layer\n",
    "# lengthW_fc1 = int(h_pool2.shape[1] * h_pool2.shape[2] * h_pool2.shape[3])\n",
    "W_fc2 = weight_variable([wsize2, 1])\n",
    "b_fc2 = bias_variable([1])\n",
    "\n",
    "y_conv = tf.reshape(tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2), [-1, int(h_pool2.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_4:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentPortfolio = tf.nn.softmax(logits=y_conv)\n",
    "\n",
    "reward0 = np.multiply(currentPortfolio, y_)\n",
    "reward = tf.reduce_sum(reward0, 1)\n",
    "\n",
    "# calculate return including transaction cost\n",
    "\n",
    "c = np.zeros(K+1) + 0.0001 # transaction cost coefficients\n",
    "\n",
    "flag = 0\n",
    "for j in xrange(K):\n",
    "    tmp = currentPortfolio[:,j]-previousPortfolio[:,j]*tf.divide(tf.exp(x[:,j,0]),(1.0+previousReturn))\n",
    "    reward = reward - c[j+1] * np.abs(tmp)\n",
    "    if tmp != 0:\n",
    "        flag = 1\n",
    "if flag:\n",
    "    reward = reward - c[0]\n",
    "\n",
    "reward_minus = -tf.reduce_prod(reward+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(reward_minus)\n",
    "test_step = tf.train.AdamOptimizer(1e-4).minimize(reward_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08842612705\n",
      "1.08852737435\n",
      "1.08862734679\n",
      "1.08871106807\n",
      "1.08881113346\n",
      "1.08891850845\n",
      "1.08903026431\n",
      "1.0891544457\n",
      "1.08928967363\n",
      "1.08943921847\n",
      "1.0896064447\n",
      "1.08979271118\n",
      "1.09000238226\n",
      "1.09023700976\n",
      "1.09049943243\n",
      "1.09079401996\n",
      "1.09112152996\n",
      "1.09148344205\n",
      "1.09189542534\n",
      "1.09235340437\n",
      "1.09286425663\n",
      "1.09343309278\n",
      "1.0940527559\n",
      "1.09474325876\n",
      "1.09549478916\n",
      "1.09633890865\n",
      "1.09726095167\n",
      "1.09820693259\n",
      "1.09919296766\n",
      "1.10035746197\n",
      "1.10158235632\n",
      "1.10285571124\n",
      "1.10431458258\n",
      "1.10581700408\n",
      "1.10757749703\n",
      "1.10916262619\n",
      "1.1110907611\n",
      "1.11285499804\n",
      "1.11495763823\n",
      "1.11712160355\n",
      "1.11942237255\n",
      "1.12182790814\n",
      "1.12436954797\n",
      "1.12711218654\n",
      "1.12983552819\n",
      "1.13285651794\n",
      "1.13596756572\n",
      "1.13923166057\n",
      "1.14267770218\n",
      "1.14632485426\n",
      "1.15022763319\n",
      "1.15442500291\n",
      "1.15888524636\n",
      "1.16365799536\n",
      "1.16877837482\n",
      "1.17414534622\n",
      "1.17988621315\n",
      "1.18590668589\n",
      "1.19250401322\n",
      "1.19978380723\n",
      "1.2073569795\n",
      "1.21553219823\n",
      "1.22372377188\n",
      "1.23234374639\n",
      "1.24148426289\n",
      "1.25103003371\n",
      "1.26108605293\n",
      "1.2715852726\n",
      "1.28269816547\n",
      "1.29441816287\n",
      "1.30673715963\n",
      "1.31959239474\n",
      "1.33316510328\n",
      "1.34740798733\n",
      "1.36215284582\n",
      "1.37770882183\n",
      "1.39387411171\n",
      "1.41077385534\n",
      "1.42821485113\n",
      "1.44633933886\n",
      "1.46468585987\n",
      "1.48356520572\n",
      "1.50277177654\n",
      "1.52228560901\n",
      "1.54199049719\n",
      "1.56194546427\n",
      "1.58157872973\n",
      "1.60132774032\n",
      "1.62086524061\n",
      "1.6402146902\n",
      "1.65919458192\n",
      "1.67782234502\n",
      "1.69611590846\n",
      "1.71375449084\n",
      "1.73097781186\n",
      "1.74756259972\n",
      "1.7635927122\n",
      "1.77914059214\n",
      "1.79404720674\n",
      "1.80822708409\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    B = 1\n",
    "    for _ in range(100):\n",
    "        PrePortfolio = np.zeros([B,K])\n",
    "        PreReturn = np.zeros(B)\n",
    "        train_accumulate = 1\n",
    "        for i in range(len(logReturn_x0)/2/B):\n",
    "            historicalData = logReturn_x[i*B:(i+1)*B]\n",
    "            compareReturn = logReturn_x0[i*B:(i+1)*B]\n",
    "            currentReturn = sess.run(reward, {x: historicalData, y_: compareReturn, \\\n",
    "                                      previousPortfolio: PrePortfolio, previousReturn: PreReturn} )\n",
    "            train_accumulate = train_accumulate * (sess.run(-reward_minus,\\\n",
    "                                                {x: historicalData,y_: compareReturn, \\\n",
    "                                                 previousPortfolio: PrePortfolio, previousReturn: PreReturn}))\n",
    "            train_step.run(feed_dict={x: historicalData, y_: compareReturn,\\\n",
    "                                  previousPortfolio: PrePortfolio, previousReturn: PreReturn})\n",
    "            prePortfolio = sess.run(currentPortfolio, \\\n",
    "                               {x: historicalData,y_: compareReturn, \\\n",
    "                                                 previousPortfolio: PrePortfolio, previousReturn: PreReturn})\n",
    "            preReturn = currentReturn\n",
    "        print train_accumulate\n",
    "    test_accumulate = 1 \n",
    "    PrePortfolio = np.zeros([B,K])\n",
    "    PreReturn = np.zeros(B)\n",
    "    for i in range(len(logReturn_x0)/2/B+1, len(logReturn_x0)/B):\n",
    "        historicalData = logReturn_x[i*B:(i+1)*B]\n",
    "        compareReturn = logReturn_x0[i*B:(i+1)*B]\n",
    "        currentReturn = sess.run(reward, {x: historicalData, y_: compareReturn, \\\n",
    "                                      previousPortfolio: PrePortfolio, previousReturn: PreReturn} )\n",
    "        test_accumulate = test_accumulate * (sess.run(-reward_minus,\\\n",
    "                                                {x: historicalData,y_: compareReturn, \\\n",
    "                                                 previousPortfolio: PrePortfolio, previousReturn: PreReturn}))\n",
    "        test_step.run(feed_dict={x: historicalData, y_: compareReturn,\\\n",
    "                                  previousPortfolio: PrePortfolio, previousReturn: PreReturn})\n",
    "        prePortfolio = sess.run(currentPortfolio, \\\n",
    "                               {x: historicalData,y_: compareReturn, \\\n",
    "                                                 previousPortfolio: PrePortfolio, previousReturn: PreReturn})\n",
    "        preReturn = currentReturn\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.80822708409 2.4294877686 1.34357448241\n"
     ]
    }
   ],
   "source": [
    "print train_accumulate,test_accumulate, test_accumulate/train_accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print len(logReturn_x0)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
